{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38464bitvenvvenv2110f9431e5d4fb88840d239d72385ed",
   "display_name": "Python 3.8.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([units,1]), name=(name + '_b'))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_norm:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W')) \n",
    "        self.activation = activation\n",
    "\n",
    "        self.gamma = tf.Variable(tf.ones([units, 1]), name=(name + '_gamma'))\n",
    "        self.beta = tf.Variable(tf.ones([units, 1]), name=(name + '_beta'))\n",
    "        self.mu_test = tf.Variable(tf.zeros([units,1]))\n",
    "        self.sigma_test = tf.Variable(tf.ones([units,1]))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        e = 10**-8\n",
    "\n",
    "        Z = tf.linalg.matmul(self.W,X)\n",
    "\n",
    "        if training:\n",
    "            mu = tf.math.reduce_mean(Z, axis=1, keepdims=True)\n",
    "            sigma = tf.math.reduce_variance(Z - mu, axis=1, keepdims=True)\n",
    "            sigma = tf.math.sqrt(sigma + e)\n",
    "\n",
    "            self.mu_test.assign(0.95*self.mu_test + 0.05*mu)\n",
    "            self.sigma_test.assign(0.95*self.sigma_test + 0.05*sigma)\n",
    "        else:\n",
    "            mu = self.mu_test\n",
    "            sigma = self.sigma_test\n",
    "\n",
    "        Z = (Z - mu) / sigma\n",
    "        Z = self.gamma * Z + self.beta\n",
    "\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, keep_prob):\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        if not training:\n",
    "            return X\n",
    "        X = tf.nn.dropout(X, 1-self.keep_prob)\n",
    "        # D = tf.random.uniform(tf.shape(X)) < self.keep_prob\n",
    "        # D = tf.cast(D, dtype=tf.float32)\n",
    "        # X = X * D\n",
    "        # X = X / self.keep_prob\n",
    "        return X\n",
    "\n",
    "    def get_vars(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, classes, input_size, name):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.W = tf.Variable(tf.random.normal([classes, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([classes,1]), name=(name + '_b'))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "\n",
    "        T = tf.math.exp(Z)\n",
    "        # A = T / tf.math.reduce_sum(T, axis=0)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer_names, units, input_size, cost_function):\n",
    "        self.layers = self._create_layers(layer_names, units, input_size)\n",
    "        self.cost = cost_function\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, X, training=False):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, training)\n",
    "        return X\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, epochs, X_batches_train, Y_batches_train, X_dev, Y_dev, optimizer, callbacks=[]):\n",
    "        for epoch in tf.range(epochs):\n",
    "            #training\n",
    "            cost_train = 0.\n",
    "            i = 0\n",
    "            for X_train, Y_train in zip(X_batches_train, Y_batches_train):       \n",
    "                cost = self.__training_step__(X_train, Y_train, optimizer)\n",
    "                cost_train = (cost_train * i + cost) / (i+1)\n",
    "                i += 1\n",
    "\n",
    "            #validation\n",
    "            prediction = self.forward(X_dev, training=False)\n",
    "            cost_dev = self.cost(Y_dev, prediction)\n",
    "\n",
    "            for callback in callbacks:\n",
    "                callback(epoch, cost_train, cost_dev)         \n",
    "\n",
    "    @tf.function\n",
    "    def __training_step__(self, X_train, Y_train, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            variables = self.__get_variables__()\n",
    "            tape.watch(variables)\n",
    "            prediction = self.forward(X_train, training=True)\n",
    "            cost = self.cost(Y_train, prediction)\n",
    "            grads = tape.gradient(cost, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        return cost\n",
    "\n",
    "    def __get_variables__(self):\n",
    "        variables = []\n",
    "        for layer in self.layers:\n",
    "            var = layer.get_vars()\n",
    "            if x != None:\n",
    "                variables += var\n",
    "        return variables\n",
    "\n",
    "    def __create_layers__(layer_names, units, input_size):\n",
    "        units.insert(0, input_size)\n",
    "        i = 1\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "            if layer_name == 'dense':\n",
    "                layer = Dense(units[i], units[i-1], tf.nn.sigmoid, 'Dense' + str(i+1))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'batch_norm':\n",
    "                layer = Batch_norm(units[i], units[i-1], tf.nn.sigmoid, 'Batch_norm' + str(i+1))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'dropout':\n",
    "                layer = Dropout(keep_prob)\n",
    "                layers.append(layer)\n",
    "\n",
    "            elif layer_name == 'softmax':  \n",
    "                layer = Softmax(units[i], units[i-1], 'Dense' + str(i+1))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "        return layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate, beta_v = 0.9, beta_s = 0.999):\n",
    "        self.v = []\n",
    "        self.s = []\n",
    "        self.iteration = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_v = beta_v\n",
    "        self.beta_s = beat_s\n",
    "\n",
    "    def apply_gradients(self, grads_vars):\n",
    "        self.iteration += 1\n",
    "        grads, variables = grads_vars\n",
    "\n",
    "        for i in tf.range(tf.size(grads)):\n",
    "            if tf.size(self.v) <= i:\n",
    "                self.v.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "            if tf.size(self.s) <= i:\n",
    "                self.s.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "\n",
    "            self.v[i].assign(self.beta_v*self.v[i] + (1-self.beta_v)*grads[i])\n",
    "            v_corrected = self.v[i] / (1 - tf.math.pow(self.beta_v,self.iteration))\n",
    "\n",
    "            self.s[i].assign(self.beta_s*self.s[i] + (1-self.beta_s)* tf.math.square(grads[i]))\n",
    "            s_corrected = self.s[i] / (1 - tf.math.pow(self.beta_s,self.iteration))\n",
    "\n",
    "            change = self.learning_rate * (v_corrected/(tf.math.sqrt(s_corrected) + 10**-8))\n",
    "\n",
    "            variables[i].assign_add(-change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_step_template(X_train_batches, Y_train_batches, X_test, Y_test, layers, optymizer, learning_rate):\n",
    "#     n = X_train_batches.shape[0]\n",
    "#     cost_train = 0.\n",
    "#     for i in tf.range(n):\n",
    "#         X = X_train_batches[i].to_tensor()\n",
    "#         Y = Y_train_batches[i].to_tensor()\n",
    "#         i = tf.cast(i, tf.float32)\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             params = get_vars(layers)\n",
    "#             tape.watch(params)\n",
    "#             A = forward_pass(X, layers, training=True)\n",
    "#             cost = Cost(A, Y)\n",
    "#             cost_train = (cost_train * i + cost) / (i+1)\n",
    "#             grads = tape.gradient(cost, params)\n",
    "#         optymizer.update_params(params, grads, learning_rate)\n",
    "#     A_test = forward_pass(X_test, layers, False)\n",
    "#     cost_test = Cost(A_test, Y_test)\n",
    "#     cost_train = tf.math.reduce_mean(cost_train)\n",
    "#     return (cost_train, cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(A, Y):\n",
    "    loss = tf.math.log(A)\n",
    "    loss = Y * loss\n",
    "    loss = -tf.math.reduce_sum(loss, axis=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost(A, Y):\n",
    "    losses = Loss(A, Y)\n",
    "    cost = tf.reduce_mean(losses)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size):\n",
    "    mini_batches = []\n",
    "    n = int(data.shape[1] / batch_size)\n",
    "    for i in range(n):\n",
    "        mini_batches.append(data[:,i*batch_size:(i+1)*batch_size])\n",
    "    mini_batches.append(data[:,-(data.shape[1] % batch_size):])\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4df4ae912a7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mx_train_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0my_train_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "y_train = tf.one_hot(y_train, depth=10, axis=0)\n",
    "y_test = tf.one_hot(y_test, depth=10, axis=0)\n",
    "\n",
    "x_train = tf.reshape(x_train, [784, -1])\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "\n",
    "x_test = tf.reshape(x_test, [784, -1])\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "x_train = x_test[:,:10000].numpy()\n",
    "y_train = y_train[:,:10000].numpy()\n",
    "\n",
    "x_test = x_test[:,:1000]\n",
    "y_test = y_test[:,:1000]\n",
    "\n",
    "\n",
    "x_train_batches = create_batches(x_train, batch_size)\n",
    "y_train_batches = create_batches(y_train, batch_size)\n",
    "\n",
    "# x_train_batches = tf.ragged.constant(x_train_batches)\n",
    "# y_train_batches = tf.ragged.constant(y_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['dense', 'batch_norm', 'dense', 'softmax']\n",
    "hidden_units = [16,8,8,10]\n",
    "input_size = 784\n",
    "hidden_units.insert(0, input_size)\n",
    "\n",
    "learning_rate = tf.constant(0.001, dtype=tf.float32)\n",
    "keep_prob = 0.9\n",
    "\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "m_train = x_train.shape[1]\n",
    "m_test = x_test.shape[1]\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers = create_layers()\n",
    "costs = []\n",
    "optimizer = Adam()\n",
    "\n",
    "training_step = tf.function(training_step_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "in user code:\n\n    <ipython-input-30-e397c0f141f2>:18 training_step_template  *\n        cost_train = (cost_train * i + cost) / (i+1)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:984 binary_op_wrapper\n        return func(x, y, name=name)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1283 _mul_dispatch\n        return gen_math_ops.mul(x, y, name=name)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6091 mul\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:503 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3d93746aa39d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 505\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    506\u001b[0m             *args, **kwds))\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 2657\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-30-e397c0f141f2>:18 training_step_template  *\n        cost_train = (cost_train * i + cost) / (i+1)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:984 binary_op_wrapper\n        return func(x, y, name=name)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1283 _mul_dispatch\n        return gen_math_ops.mul(x, y, name=name)\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6091 mul\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    d:\\Programming\\NeuralNetworks\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:503 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "print(training_step(x_train_batches, y_train_batches, x_test, y_test, layers, optimizer, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tf.range(1000, dtype=tf.float32):\n",
    "    cost = training_step(x_train_batches, y_train_batches, x_test, y_test, layers, optimizer, learning_rate)\n",
    "    costs.append(cost)\n",
    "    if i%100 == 0:\n",
    "        print('Epoch {0}: {1}'.format(i, cost))\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = forward_pass(x_train, layers, False)\n",
    "a = tf.argmax(prediction, axis=0)\n",
    "y = tf.argmax(y_train, axis=0)\n",
    "\n",
    "b = a==y\n",
    "b = tf.cast(b, tf.float32)\n",
    "b = tf.reduce_sum(b)\n",
    "print(b/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}