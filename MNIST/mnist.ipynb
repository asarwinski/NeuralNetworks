{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38464bitvenvvenv2110f9431e5d4fb88840d239d72385ed",
   "display_name": "Python 3.8.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([units,1]), name=(name + '_b'))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_norm:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W')) \n",
    "        self.activation = activation\n",
    "\n",
    "        self.gamma = tf.Variable(tf.ones([units, 1]), name=(name + '_gamma'))\n",
    "        self.beta = tf.Variable(tf.ones([units, 1]), name=(name + '_beta'))\n",
    "        self.mu_test = tf.Variable(tf.zeros([units,1]))\n",
    "        self.sigma_test = tf.Variable(tf.ones([units,1]))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        e = 10**-8\n",
    "\n",
    "        Z = tf.linalg.matmul(self.W,X)\n",
    "\n",
    "        if training:\n",
    "            mu = tf.math.reduce_mean(Z, axis=1, keepdims=True)\n",
    "            sigma = tf.math.reduce_variance(Z - mu, axis=1, keepdims=True)\n",
    "            sigma = tf.math.sqrt(sigma + e)\n",
    "\n",
    "            self.mu_test.assign(0.95*self.mu_test + 0.05*mu)\n",
    "            self.sigma_test.assign(0.95*self.sigma_test + 0.05*sigma)\n",
    "        else:\n",
    "            mu = self.mu_test\n",
    "            sigma = self.sigma_test\n",
    "\n",
    "        Z = (Z - mu) / sigma\n",
    "        Z = self.gamma * Z + self.beta\n",
    "\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, keep_prob):\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        if not training:\n",
    "            return X\n",
    "        X = tf.nn.dropout(X, 1-self.keep_prob)\n",
    "        # D = tf.random.uniform(tf.shape(X)) < self.keep_prob\n",
    "        # D = tf.cast(D, dtype=tf.float32)\n",
    "        # X = X * D\n",
    "        # X = X / self.keep_prob\n",
    "        return X\n",
    "\n",
    "    def get_vars(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, classes, input_size, name):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.W = tf.Variable(tf.random.normal([classes, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([classes,1]), name=(name + '_b'))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "\n",
    "        T = tf.math.exp(Z)\n",
    "        A = T / tf.math.reduce_sum(T, axis=0)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer_names, units, input_size, cost_function):\n",
    "        self.layers = self.__create_layers__(layer_names, units, input_size)\n",
    "        self.cost = cost_function\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, X, training=False):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, training)\n",
    "        return X\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, X_batches_train, Y_batches_train, X_dev, Y_dev, optimizer):\n",
    "        print('Traceing...')\n",
    "        #training\n",
    "        cost_train = 0.\n",
    "        i = 0\n",
    "        for X_train, Y_train in zip(X_batches_train, Y_batches_train):       \n",
    "            cost = self.__training_step__(X_train, Y_train, optimizer)\n",
    "            cost_train = (cost_train * i + cost) / (i+1)\n",
    "            i += 1\n",
    "\n",
    "        #validation\n",
    "        prediction = self.forward(X_dev, training=False)\n",
    "        cost_dev = self.cost(Y_dev, prediction) \n",
    "\n",
    "        return cost_train, cost_dev\n",
    "\n",
    "    @tf.function\n",
    "    def __training_step__(self, X_train, Y_train, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            variables = self.__get_variables__()\n",
    "            tape.watch(variables)\n",
    "            prediction = self.forward(X_train, training=True)\n",
    "            cost = self.cost(Y_train, prediction)\n",
    "            grads = tape.gradient(cost, variables)\n",
    "        optimizer.apply_gradients(grads, variables)\n",
    "        return cost\n",
    "\n",
    "    def __get_variables__(self):\n",
    "        variables = []\n",
    "        for layer in self.layers:\n",
    "            var = layer.get_vars()\n",
    "            if var != None:\n",
    "                variables += var\n",
    "        return variables\n",
    "\n",
    "    def __create_layers__(self, layer_names, units, input_size):\n",
    "        units.insert(0, input_size)\n",
    "        layers = []\n",
    "        i = 1\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "            if layer_name == 'dense':\n",
    "                layer = Dense(units[i], units[i-1], tf.nn.sigmoid, 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'batch_norm':\n",
    "                layer = Batch_norm(units[i], units[i-1], tf.nn.sigmoid, 'Batch_norm' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'dropout':\n",
    "                layer = Dropout(keep_prob)\n",
    "                layers.append(layer)\n",
    "\n",
    "            elif layer_name == 'softmax':  \n",
    "                layer = Softmax(units[i], units[i-1], 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "        return layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate, beta_v = 0.9, beta_s = 0.999):\n",
    "        self.v = []\n",
    "        self.s = []\n",
    "        self.iteration = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_v = beta_v\n",
    "        self.beta_s = beta_s\n",
    "\n",
    "    def apply_gradients(self, grads, variables):\n",
    "        self.iteration += 1\n",
    "\n",
    "        for i in range(len(grads)):\n",
    "            if len(self.v) <= i:\n",
    "                self.v.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "            if len(self.s) <= i:\n",
    "                self.s.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "\n",
    "            self.v[i].assign(self.beta_v*self.v[i] + (1-self.beta_v)*grads[i])\n",
    "            v_corrected = self.v[i] / (1 - tf.math.pow(self.beta_v,self.iteration))\n",
    "\n",
    "            self.s[i].assign(self.beta_s*self.s[i] + (1-self.beta_s)* tf.math.square(grads[i]))\n",
    "            s_corrected = self.s[i] / (1 - tf.math.pow(self.beta_s,self.iteration))\n",
    "\n",
    "            change = self.learning_rate * (v_corrected/(tf.math.sqrt(s_corrected) + 10**-8))\n",
    "\n",
    "            variables[i].assign_add(-change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(Y, A):\n",
    "    loss = tf.math.log(A)\n",
    "    loss = Y * loss\n",
    "    loss = -tf.math.reduce_sum(loss, axis=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost(Y, A):\n",
    "    losses = Loss(Y, A)\n",
    "    cost = tf.reduce_mean(losses)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size):\n",
    "    mini_batches = []\n",
    "    n = int(data.shape[1] / batch_size)\n",
    "    for i in range(n):\n",
    "        mini_batches.append(data[:,i*batch_size:(i+1)*batch_size])\n",
    "    mini_batches.append(data[:,-(data.shape[1] % batch_size):])\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(batch_size):\n",
    "    (x_train, y_train), (x_dev, y_dev) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train / 255\n",
    "    x_dev = x_dev / 255\n",
    "    y_train = tf.one_hot(y_train, depth=10, axis=0)\n",
    "    y_dev = tf.one_hot(y_dev, depth=10, axis=0)\n",
    "\n",
    "    x_train = tf.reshape(x_train, [-1, image_height*image_width])\n",
    "    x_train = tf.linalg.matrix_transpose(x_train)\n",
    "    x_train = tf.cast(x_train, tf.float32)\n",
    "\n",
    "    x_dev = tf.reshape(x_dev, [-1, image_height*image_width])\n",
    "    x_dev = tf.linalg.matrix_transpose(x_dev)\n",
    "    x_dev = tf.cast(x_dev, tf.float32)\n",
    "\n",
    "    x_train = x_train[:,:100000]\n",
    "    y_train = y_train[:,:100000]\n",
    "\n",
    "    x_dev = x_dev[:,:1000]\n",
    "    y_dev = y_dev[:,:1000]\n",
    "\n",
    "\n",
    "    x_train_batches = create_batches(x_train, batch_size)\n",
    "    y_train_batches = create_batches(y_train, batch_size)\n",
    "\n",
    "    # x_train_batches = tf.ragged.constant(x_train_batches)\n",
    "    # y_train_batches = tf.ragged.constant(y_train_batches)\n",
    "    return (x_train_batches, y_train_batches), (x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "def print_progress(epoch, cost_train, cost_dev):\n",
    "    if epoch % 100 == 0:\n",
    "        tf.print('Epoch ', epoch, ': train - ', cost_train, ' dev - ', cost_dev)\n",
    "\n",
    "def log_cost(epoch, cost_train, cost_dev):\n",
    "    tf.py_function(lambda x,y: cost_history.append([x,y]), inp=[cost_train, cost_dev], Tout=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "(x_batches_train, y_batches_train), (x_dev, y_dev) = get_training_data(batch_size)\n",
    "x_train = tf.concat(x_batches_train, axis=1)\n",
    "y_train = tf.concat(y_batches_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    ['dense', 'dense', 'dense', 'softmax'],\n",
    "    ['dense', 'batch_norm', 'batch_norm', 'softmax'],\n",
    "    ['dense', 'dense', 'batch_norm', 'softmax'],\n",
    "    ['dense', 'batch_norm', 'dense', 'softmax']]\n",
    "\n",
    "units = [8,4,4,10]\n",
    "input_size = 784\n",
    "cost_history = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "models = []\n",
    "for layers in layer_names:\n",
    "    models.append(Model(layers, units[:], input_size, cost_function=Cost))\n",
    "\n",
    "optimizers = [Adam(learning_rate) for _ in range(4)]\n",
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "for optimizer in optimizers:\n",
    "    optimizer.learning_rate = learning_rate\n",
    "epochs = tf.constant(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    costs = []\n",
    "    for model, optimizer in zip(models, optimizers):\n",
    "        cost_train, cost_dev = model.train(x_batches_train, y_batches_train, x_dev, y_dev, optimizer)\n",
    "        costs.append(cost_train)\n",
    "        costs.append(cost_dev)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        #print('Epoch {0:4d}: cost_train-{1:.7f} cost_dev-{2:.7f}'.format(total_epochs, cost_train, cost_dev))\n",
    "        print('Epoch {0:4d}'.format(total_epochs))\n",
    "    total_epochs += 1\n",
    "\n",
    "    cost_history.append(costs)\n",
    "\n",
    "plots = np.asarray(cost_history)\n",
    "plots = np.transpose(plots)\n",
    "m = plots.shape[1]\n",
    "plots = np.reshape(plots, [-1,2,m])\n",
    "models_names = ['full dense', 'hidden_batch_norm', 'first_dense', 'first_batch_norm']\n",
    "colors = ['red', 'blue', 'orange', 'gray']\n",
    "for cost_train, cost_dev, name, color in zip(plots[:,0], plots[:,1], models_names, colors):\n",
    "    plt.plot(cost_train, color=color, label=name+'-train')\n",
    "    plt.plot(cost_dev, color=color, label=name+'-dev', linestyle='dashed')\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = model.forward(tf.concat(x_batches_train, axis=1))\n",
    "a = tf.argmax(prediction, axis=0)\n",
    "y = tf.argmax(tf.concat(y_batches_train, axis=1), axis=0)\n",
    "\n",
    "b = a==y\n",
    "b = tf.cast(b, tf.float32)\n",
    "b = tf.reduce_sum(b)\n",
    "print(b/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = x_batches_train[0]\n",
    "\n",
    "prediction = model.forward(x_test)\n",
    "prediction = tf.argmax(prediction, axis=0)\n",
    "\n",
    "x_test = tf.reshape(x_test, [image_height, image_width, -1])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 4\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[:,:, i+offset], cmap=plt.cm.binary)\n",
    "    plt.xlabel(prediction[i+offset].numpy())\n",
    "plt.show()"
   ]
  }
 ]
}