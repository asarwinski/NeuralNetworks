{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38464bitvenvvenv2110f9431e5d4fb88840d239d72385ed",
   "display_name": "Python 3.8.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "image_height = 28\n",
    "image_width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([units,1]), name=(name + '_b'))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_norm:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W')) \n",
    "        self.activation = activation\n",
    "\n",
    "        self.gamma = tf.Variable(tf.ones([units, 1]), name=(name + '_gamma'))\n",
    "        self.beta = tf.Variable(tf.ones([units, 1]), name=(name + '_beta'))\n",
    "        self.mu_test = tf.Variable(tf.zeros([units,1]))\n",
    "        self.sigma_test = tf.Variable(tf.ones([units,1]))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        e = 10**-8\n",
    "\n",
    "        Z = tf.linalg.matmul(self.W,X)\n",
    "\n",
    "        if training:\n",
    "            mu = tf.math.reduce_mean(Z, axis=1, keepdims=True)\n",
    "            sigma = tf.math.reduce_variance(Z - mu, axis=1, keepdims=True)\n",
    "            sigma = tf.math.sqrt(sigma + e)\n",
    "\n",
    "            self.mu_test.assign(0.95*self.mu_test + 0.05*mu)\n",
    "            self.sigma_test.assign(0.95*self.sigma_test + 0.05*sigma)\n",
    "        else:\n",
    "            mu = self.mu_test\n",
    "            sigma = self.sigma_test\n",
    "\n",
    "        Z = (Z - mu) / sigma\n",
    "        Z = self.gamma * Z + self.beta\n",
    "\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.gamma, self.beta]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, keep_prob):\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        if not training:\n",
    "            return X\n",
    "        X = tf.nn.dropout(X, 1-self.keep_prob)\n",
    "        # D = tf.random.uniform(tf.shape(X)) < self.keep_prob\n",
    "        # D = tf.cast(D, dtype=tf.float32)\n",
    "        # X = X * D\n",
    "        # X = X / self.keep_prob\n",
    "        return X\n",
    "\n",
    "    def get_vars(self):\n",
    "        return None\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, classes, input_size, name):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.W = tf.Variable(tf.random.normal([classes, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([classes,1]), name=(name + '_b'))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "\n",
    "        T = tf.math.exp(Z)\n",
    "        A = T / tf.math.reduce_sum(T, axis=0)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer_names, units, input_size, cost_function, name=\"model\"):\n",
    "        self.layers = self.__create_layers__(layer_names, units, input_size)\n",
    "        self.cost = cost_function\n",
    "        self.input_size = input_size\n",
    "        self.name = name\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, X, training=False, testing=False):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, training)\n",
    "            if testing:\n",
    "                tf.print(X)\n",
    "        return X\n",
    "    \n",
    "    def train(self, train_ds, dev_ds, optimizer, weights_regularization=None):\n",
    "        #training\n",
    "        self.variables = self.__get_variables__()\n",
    "        numerator = 0.\n",
    "        denominator = 0.\n",
    "        for X, Y in train_ds: \n",
    "            cost = self.__training_step__(X, Y, optimizer)\n",
    "            m = tf.shape(X)[1]\n",
    "            m = tf.cast(m, tf.float32)\n",
    "            numerator = numerator + cost * m\n",
    "            denominator = denominator + m\n",
    "        cost_train = numerator / denominator\n",
    "\n",
    "        #validation\n",
    "        numerator = 0.\n",
    "        denominator = 0.\n",
    "        for X, Y in dev_ds:\n",
    "            prediction = self.forward(X, training=False)\n",
    "            cost = self.cost(Y, prediction) \n",
    "            m = tf.shape(X)[1]\n",
    "            m = tf.cast(m, tf.float32)\n",
    "            numerator = numerator + cost * m\n",
    "            denominator = denominator + m\n",
    "        cost_dev = numerator / denominator\n",
    "\n",
    "        return cost_train, cost_dev\n",
    "\n",
    "    @tf.function\n",
    "    def __training_step__(self, X_train, Y_train, optimizer):\n",
    "        print('Traceing...')\n",
    "        with tf.GradientTape() as tape:           \n",
    "            prediction = self.forward(X_train, training=True)\n",
    "            cost = self.cost(Y_train, prediction)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "   \n",
    "        optimizer.apply_gradients(grads, self.variables)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def __get_weights__(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if weights != None:\n",
    "                weights += (weights)\n",
    "        return weights\n",
    "        \n",
    "    def __get_variables__(self):\n",
    "        variables = []\n",
    "        for layer in self.layers:\n",
    "            var = layer.get_vars()\n",
    "            if var != None:\n",
    "                variables += var\n",
    "        return variables\n",
    "\n",
    "    def __create_layers__(self, layer_names, units, input_size):\n",
    "        units.insert(0, input_size)\n",
    "        layers = []\n",
    "        i = 1\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "            if layer_name == 'dense':\n",
    "                layer = Dense(units[i], units[i-1], tf.nn.relu, 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'batch_norm':\n",
    "                layer = Batch_norm(units[i], units[i-1], tf.nn.relu, 'Batch_norm' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'dropout':\n",
    "                layer = Dropout(keep_prob)\n",
    "                layers.append(layer)\n",
    "\n",
    "            elif layer_name == 'softmax':  \n",
    "                layer = Softmax(units[i], units[i-1], 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "        return layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate, beta_v = 0.9, beta_s = 0.999):\n",
    "        self.v = []\n",
    "        self.s = []\n",
    "        self.iteration = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_v = beta_v\n",
    "        self.beta_s = beta_s\n",
    "\n",
    "    def apply_gradients(self, grads, variables):\n",
    "        self.iteration += 1\n",
    "\n",
    "        for i in range(len(grads)):\n",
    "            if len(self.v) <= i:\n",
    "                self.v.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "            if len(self.s) <= i:\n",
    "                self.s.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "\n",
    "            self.v[i].assign(self.beta_v*self.v[i] + (1-self.beta_v)*grads[i])\n",
    "            v_corrected = self.v[i] / (1 - tf.math.pow(self.beta_v,self.iteration))\n",
    "\n",
    "            self.s[i].assign(self.beta_s*self.s[i] + (1-self.beta_s)* tf.math.square(grads[i]))\n",
    "            s_corrected = self.s[i] / (1 - tf.math.pow(self.beta_s,self.iteration))\n",
    "\n",
    "            change = self.learning_rate * (v_corrected/(tf.math.sqrt(s_corrected) + 10**-8))\n",
    "\n",
    "            variables[i].assign_add(-change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(Y, A):\n",
    "    loss = tf.math.log(A + 10**-8)\n",
    "    loss = Y * loss\n",
    "    loss = -tf.math.reduce_sum(loss, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost(Y, A):\n",
    "    losses = Loss(Y, A)\n",
    "    cost = tf.reduce_mean(losses)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(img):\n",
    "    img = tf.reshape(img, [1,image_height,image_width,1])\n",
    "    kernel = tf.ones([3,3,1,1], dtype=tf.float32)\n",
    "    mask = tf.nn.conv2d(img, kernel, strides=1, padding='SAME')\n",
    "    mask = mask / 9\n",
    "    img = img + mask\n",
    "    img = tf.math.minimum(img, 1)\n",
    "    return tf.reshape(img, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(img):\n",
    "    mask = img > 0.7\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return img * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(batch_size):\n",
    "    (x_train, y_train), (x_dev, y_dev) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.reshape(x,[-1])/255, tf.one_hot(y,10)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.interleave(lambda x,y: tf.data.Dataset.from_tensor_slices(([x, blur(x), sharpen(x)], [y,y,y])),block_length=3, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.linalg.matrix_transpose(x), tf.linalg.matrix_transpose(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.cache()\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dev_ds = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "    dev_ds = dev_ds.map(lambda x,y: (tf.reshape(x,[-1])/255, tf.one_hot(y,10)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.interleave(lambda x,y: tf.data.Dataset.from_tensor_slices(([x, blur(x), sharpen(x)], [y,y,y])),block_length=3, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.batch(batch_size)\n",
    "    dev_ds = dev_ds.map(lambda x,y: (tf.linalg.matrix_transpose(x), tf.linalg.matrix_transpose(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.cache()\n",
    "    dev_ds = dev_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_ds, dev_ds = get_training_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    ['batch_norm', 'batch_norm', 'batch_norm', 'softmax'],\n",
    "    ['batch_norm', 'batch_norm', 'softmax']]\n",
    "#layer_names = [['batch_norm', 'batch_norm', 'softmax']]\n",
    "\n",
    "units = [\n",
    "    [8,4,4,10],\n",
    "    [8,4,10]]\n",
    "\n",
    "names = ['4-layers', '3-layers']\n",
    "\n",
    "input_size = 784\n",
    "cost_history = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "models = []\n",
    "for layers, u, name in zip(layer_names, units, names):\n",
    "    models.append(Model(layers, u[:], input_size, cost_function=Cost, name=name))\n",
    "\n",
    "#optimizers = [Adam(learning_rate) for _ in range(4)]\n",
    "optimizers = [Adam(learning_rate) for _ in range(len(layer_names))]\n",
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for optimizer in optimizers:\n",
    "    optimizer.learning_rate = learning_rate\n",
    "epochs = tf.constant(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    costs = []\n",
    "    for model, optimizer in zip(models, optimizers):\n",
    "        start = tf.timestamp()\n",
    "        cost_train, cost_dev = model.train(train_ds, dev_ds, optimizer)\n",
    "        end = tf.timestamp()\n",
    "        costs.append(cost_train)\n",
    "        costs.append(cost_dev)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch {0:4d}: cost_train={1:.7f} cost_dev={2:.7f}, time={3:.5f}'.format(total_epochs, cost_train, cost_dev,end-start))\n",
    "        #print('Epoch {0:4d}     {1}'.format(total_epochs, end-start))\n",
    "    total_epochs += 1\n",
    "\n",
    "    cost_history.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots = np.asarray(cost_history)\n",
    "plots = np.transpose(plots)\n",
    "m = plots.shape[1]\n",
    "plots = np.reshape(plots, [-1,2,m])\n",
    "for cost_train, cost_dev, model, color in zip(plots[:,0], plots[:,1], models, colors):\n",
    "    i = randint(0, len(colors)-1)\n",
    "    plt.plot(cost_train, color=colors[i], label=model.name+'-train')\n",
    "    plt.plot(cost_dev, color=colors[i], label=model.name+'-dev', linestyle='dashed')\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc accuracy\n",
    "for model in models:\n",
    "    print(model.name)\n",
    "    m = 0.\n",
    "    correct = 0.\n",
    "    for X,Y in train_ds:\n",
    "        prediction = model.forward(X)\n",
    "        prediction = tf.argmax(prediction, axis=0)\n",
    "        Y = tf.argmax(Y, axis=0)\n",
    "        temp = prediction == Y\n",
    "        temp = tf.cast(temp, tf.float32)\n",
    "        correct += tf.math.reduce_sum(temp)\n",
    "        m += tf.cast(tf.shape(X)[1], tf.float32)\n",
    "    tf.print('Train accuracy:',correct/m)\n",
    "\n",
    "    m = 0.\n",
    "    correct = 0.\n",
    "    for X,Y in dev_ds:\n",
    "        prediction = model.forward(X)\n",
    "        prediction = tf.argmax(prediction, axis=0)\n",
    "        Y = tf.argmax(Y, axis=0)\n",
    "        temp = prediction == Y\n",
    "        temp = tf.cast(temp, tf.float32)\n",
    "        correct += tf.math.reduce_sum(temp)\n",
    "        m += tf.cast(tf.shape(X)[1], tf.float32)\n",
    "    tf.print('Dev accuracy:',correct/m)\n",
    "    print('_____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_ds.take(1):\n",
    "    prediction = models[0].forward(X)\n",
    "    prediction = tf.math.argmax(prediction, axis=0)\n",
    "    ims = tf.reshape(X, [image_height, image_width, -1])\n",
    "\n",
    "offset = 10\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(ims[:,:,i+offset], cmap='binary')\n",
    "    plt.xlabel(prediction[i+offset].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = tf.io.read_file('test.png')\n",
    "img = tf.image.decode_png(f)\n",
    "img = img / 255\n",
    "img = tf.reshape(img, [image_height * image_width, -1])\n",
    "print(img.shape)\n",
    "tf.print(tf.math.reduce_max(img))\n",
    "plt.imshow(tf.reshape(img,[image_height, image_width]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "        prediction = model.forward(img)\n",
    "        tf.print('Prediction:',tf.argmax(prediction, axis=0))\n",
    "        for i in prediction:\n",
    "            tf.print(i)\n",
    "        print('_________')"
   ]
  }
 ]
}