{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38464bitvenvvenv2110f9431e5d4fb88840d239d72385ed",
   "display_name": "Python 3.8.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']"
    "\n",
    "image_height = 28\n",
    "image_width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([units,1]), name=(name + '_b'))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_norm:\n",
    "    def __init__(self, units, input_size, activation, name):\n",
    "        self.W = tf.Variable(tf.random.normal([units, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W')) \n",
    "        self.activation = activation\n",
    "\n",
    "        self.gamma = tf.Variable(tf.ones([units, 1]), name=(name + '_gamma'))\n",
    "        self.beta = tf.Variable(tf.ones([units, 1]), name=(name + '_beta'))\n",
    "        self.mu_test = tf.Variable(tf.zeros([units,1]))\n",
    "        self.sigma_test = tf.Variable(tf.ones([units,1]))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        e = 10**-8\n",
    "\n",
    "        Z = tf.linalg.matmul(self.W,X)\n",
    "\n",
    "        if training:\n",
    "            mu = tf.math.reduce_mean(Z, axis=1, keepdims=True)\n",
    "            sigma = tf.math.reduce_variance(Z - mu, axis=1, keepdims=True)\n",
    "            sigma = tf.math.sqrt(sigma + e)\n",
    "\n",
    "            self.mu_test.assign(0.95*self.mu_test + 0.05*mu)\n",
    "            self.sigma_test.assign(0.95*self.sigma_test + 0.05*sigma)\n",
    "        else:\n",
    "            mu = self.mu_test\n",
    "            sigma = self.sigma_test\n",
    "\n",
    "        Z = (Z - mu) / sigma\n",
    "        Z = self.gamma * Z + self.beta\n",
    "\n",
    "        A = self.activation(Z)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.gamma, self.beta]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, keep_prob):\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        if not training:\n",
    "            return X\n",
    "        X = tf.nn.dropout(X, 1-self.keep_prob)\n",
    "        # D = tf.random.uniform(tf.shape(X)) < self.keep_prob\n",
    "        # D = tf.cast(D, dtype=tf.float32)\n",
    "        # X = X * D\n",
    "        # X = X / self.keep_prob\n",
    "        return X\n",
    "\n",
    "    def get_vars(self):\n",
    "        return None\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, classes, input_size, name):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.W = tf.Variable(tf.random.normal([classes, input_size]) * tf.math.sqrt(1/input_size), name=(name + '_W'))\n",
    "        self.b = tf.Variable(tf.zeros([classes,1]), name=(name + '_b'))\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        Z = tf.linalg.matmul(self.W,X) + self.b\n",
    "\n",
    "        T = tf.math.exp(Z)\n",
    "        A = T / tf.math.reduce_sum(T, axis=0)\n",
    "        return A\n",
    "\n",
    "    def get_vars(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer_names, units, input_size, cost_function, name=\"model\"):\n",
    "        self.layers = self.__create_layers__(layer_names, units, input_size)\n",
    "        self.cost = cost_function\n",
    "        self.input_size = input_size\n",
    "        self.name = name\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, X, training=False, testing=False):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, training)\n",
    "            if testing:\n",
    "                tf.print(X)\n",
    "        return X\n",
    "    \n",
    "    def train(self, train_ds, dev_ds, optimizer, weights_regularization=None):\n",
    "        #training\n",
    "        self.variables = self.__get_variables__()\n",
    "        numerator = 0.\n",
    "        denominator = 0.\n",
    "        for X, Y in train_ds: \n",
    "            cost = self.__training_step__(X, Y, optimizer)\n",
    "            m = tf.shape(X)[1]\n",
    "            m = tf.cast(m, tf.float32)\n",
    "            numerator = numerator + cost * m\n",
    "            denominator = denominator + m\n",
    "        cost_train = numerator / denominator\n",
    "\n",
    "        #validation\n",
    "        numerator = 0.\n",
    "        denominator = 0.\n",
    "        for X, Y in dev_ds:\n",
    "            prediction = self.forward(X, training=False)\n",
    "            cost = self.cost(Y, prediction) \n",
    "            m = tf.shape(X)[1]\n",
    "            m = tf.cast(m, tf.float32)\n",
    "            numerator = numerator + cost * m\n",
    "            denominator = denominator + m\n",
    "        cost_dev = numerator / denominator\n",
    "\n",
    "        return cost_train, cost_dev\n",
    "\n",
    "    @tf.function\n",
    "    def __training_step__(self, X_train, Y_train, optimizer):\n",
    "        print('Traceing...')\n",
    "        with tf.GradientTape() as tape:           \n",
    "            prediction = self.forward(X_train, training=True)\n",
    "            cost = self.cost(Y_train, prediction)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "   \n",
    "        optimizer.apply_gradients(grads, self.variables)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def __get_weights__(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if weights != None:\n",
    "                weights += (weights)\n",
    "        return weights\n",
    "        \n",
    "    def __get_variables__(self):\n",
    "        variables = []\n",
    "        for layer in self.layers:\n",
    "            var = layer.get_vars()\n",
    "            if var != None:\n",
    "                variables += var\n",
    "        return variables\n",
    "\n",
    "    def __create_layers__(self, layer_names, units, input_size):\n",
    "        units.insert(0, input_size)\n",
    "        layers = []\n",
    "        i = 1\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "            if layer_name == 'dense':\n",
    "                layer = Dense(units[i], units[i-1], tf.nn.relu, 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'batch_norm':\n",
    "                layer = Batch_norm(units[i], units[i-1], tf.nn.relu, 'Batch_norm' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "            elif layer_name == 'dropout':\n",
    "                layer = Dropout(keep_prob)\n",
    "                layers.append(layer)\n",
    "\n",
    "            elif layer_name == 'softmax':  \n",
    "                layer = Softmax(units[i], units[i-1], 'Dense' + str(i))\n",
    "                layers.append(layer)\n",
    "                i += 1\n",
    "\n",
    "        return layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate, beta_v = 0.9, beta_s = 0.999):\n",
    "        self.v = []\n",
    "        self.s = []\n",
    "        self.iteration = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_v = beta_v\n",
    "        self.beta_s = beta_s\n",
    "\n",
    "    def apply_gradients(self, grads, variables):\n",
    "        self.iteration += 1\n",
    "\n",
    "        for i in range(len(grads)):\n",
    "            if len(self.v) <= i:\n",
    "                self.v.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "            if len(self.s) <= i:\n",
    "                self.s.append(tf.Variable(tf.zeros(tf.shape(grads[i]))))\n",
    "\n",
    "            self.v[i].assign(self.beta_v*self.v[i] + (1-self.beta_v)*grads[i])\n",
    "            v_corrected = self.v[i] / (1 - tf.math.pow(self.beta_v,self.iteration))\n",
    "\n",
    "            self.s[i].assign(self.beta_s*self.s[i] + (1-self.beta_s)* tf.math.square(grads[i]))\n",
    "            s_corrected = self.s[i] / (1 - tf.math.pow(self.beta_s,self.iteration))\n",
    "\n",
    "            change = self.learning_rate * (v_corrected/(tf.math.sqrt(s_corrected) + 10**-8))\n",
    "\n",
    "            variables[i].assign_add(-change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(Y, A):\n",
    "    loss = tf.math.log(A + 10**-8)\n",
    "    loss = Y * loss\n",
    "    loss = -tf.math.reduce_sum(loss, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost(Y, A):\n",
    "    losses = Loss(Y, A)\n",
    "    cost = tf.reduce_mean(losses)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(img):\n",
    "    img = tf.reshape(img, [1,image_height,image_width,1])\n",
    "    kernel = tf.ones([3,3,1,1], dtype=tf.float32)\n",
    "    mask = tf.nn.conv2d(img, kernel, strides=1, padding='SAME')\n",
    "    mask = mask / 9\n",
    "    img = img + mask\n",
    "    img = tf.math.minimum(img, 1)\n",
    "    return tf.reshape(img, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(img):\n",
    "    mask = img > 0.7\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return img * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(batch_size):\n",
    "    (x_train, y_train), (x_dev, y_dev) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.reshape(x,[-1])/255, tf.one_hot(y,10)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.interleave(lambda x,y: tf.data.Dataset.from_tensor_slices(([x, blur(x), sharpen(x)], [y,y,y])),block_length=3, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.linalg.matrix_transpose(x), tf.linalg.matrix_transpose(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_ds = train_ds.cache()\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dev_ds = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "    dev_ds = dev_ds.map(lambda x,y: (tf.reshape(x,[-1])/255, tf.one_hot(y,10)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.interleave(lambda x,y: tf.data.Dataset.from_tensor_slices(([x, blur(x), sharpen(x)], [y,y,y])),block_length=3, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.batch(batch_size)\n",
    "    dev_ds = dev_ds.map(lambda x,y: (tf.linalg.matrix_transpose(x), tf.linalg.matrix_transpose(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dev_ds = dev_ds.cache()\n",
    "    dev_ds = dev_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for X, Y in train_ds.take(1):\n",
    "    for img in tf.linalg.matrix_transpose(X)[:5]:\n",
    "        img = tf.reshape(img, [1,image_height, image_width,1])\n",
    "        plt.imshow(tf.squeeze(img), cmap='gray')\n",
    "        plt.show()\n",
    "        blur_img = blur(img[:])\n",
    "        plt.imshow(tf.squeeze(blur_img), cmap='gray')\n",
    "        plt.show()\n",
    "        sharpen_img = sharpen(img[:])\n",
    "        plt.imshow(tf.squeeze(sharpen_img), cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_ds, dev_ds = get_training_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    ['batch_norm', 'batch_norm', 'batch_norm', 'softmax'],\n",
    "    ['batch_norm', 'batch_norm', 'softmax']]\n",
    "#layer_names = [['batch_norm', 'batch_norm', 'softmax']]\n",
    "\n",
    "units = [\n",
    "    [8,4,4,10],\n",
    "    [8,4,10]]\n",
    "\n",
    "names = ['4-layers', '3-layers']\n",
    "\n",
    "input_size = 784\n",
    "cost_history = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "models = []\n",
    "for layers, u, name in zip(layer_names, units, names):\n",
    "    models.append(Model(layers, u[:], input_size, cost_function=Cost, name=name))\n",
    "\n",
    "#optimizers = [Adam(learning_rate) for _ in range(4)]\n",
    "optimizers = [Adam(learning_rate) for _ in range(len(layer_names))]\n",
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for optimizer in optimizers:\n",
    "    optimizer.learning_rate = learning_rate\n",
    "epochs = tf.constant(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Traceing...\nTraceing...\nTraceing...\nTraceing...\nTraceing...\nTraceing...\nEpoch    0: cost_train=43.4873314 cost_dev=27.0772285, time=2.57472\nEpoch    1: cost_train=24.4465942 cost_dev=22.2826786, time=0.66858\nEpoch    2: cost_train=21.7655201 cost_dev=20.8439827, time=0.66886\nEpoch    3: cost_train=20.7616787 cost_dev=20.2340050, time=0.68326\nEpoch    4: cost_train=20.2086868 cost_dev=19.8916168, time=0.63772\nEpoch    5: cost_train=19.8509254 cost_dev=19.6861191, time=0.63235\nEpoch    6: cost_train=19.5943966 cost_dev=19.5493107, time=0.63942\nEpoch    7: cost_train=19.3910770 cost_dev=19.4475403, time=0.65339\nEpoch    8: cost_train=19.2231884 cost_dev=19.3653545, time=0.64185\nEpoch    9: cost_train=19.0758133 cost_dev=19.2844887, time=0.74080\nEpoch   10: cost_train=18.9514065 cost_dev=19.1992817, time=0.66996\nEpoch   11: cost_train=18.8309689 cost_dev=19.1333809, time=0.69164\nEpoch   12: cost_train=18.7277050 cost_dev=19.0635777, time=0.64867\nEpoch   13: cost_train=18.6267910 cost_dev=19.0141582, time=0.68577\nEpoch   14: cost_train=18.5334930 cost_dev=18.9580860, time=0.64100\nEpoch   15: cost_train=18.4415493 cost_dev=18.8993683, time=0.63006\nEpoch   16: cost_train=18.3563652 cost_dev=18.8431568, time=0.64494\nEpoch   17: cost_train=18.2681789 cost_dev=18.7873898, time=0.62423\nEpoch   18: cost_train=18.1909218 cost_dev=18.7294502, time=0.62557\nEpoch   19: cost_train=18.1162453 cost_dev=18.6978493, time=0.62451\nEpoch   20: cost_train=18.0419369 cost_dev=18.6619205, time=0.65936\nEpoch   21: cost_train=17.9674702 cost_dev=18.6177464, time=0.65060\nEpoch   22: cost_train=17.8977051 cost_dev=18.5768452, time=0.69352\nEpoch   23: cost_train=17.8252869 cost_dev=18.5521488, time=0.65304\nEpoch   24: cost_train=17.7590523 cost_dev=18.5251904, time=0.65802\nEpoch   25: cost_train=17.6994133 cost_dev=18.4995575, time=0.62604\nEpoch   26: cost_train=17.6453972 cost_dev=18.4719257, time=0.62622\nEpoch   27: cost_train=17.5928230 cost_dev=18.4396763, time=0.62652\nEpoch   28: cost_train=17.5399227 cost_dev=18.4166050, time=0.62850\nEpoch   29: cost_train=17.4914017 cost_dev=18.3956890, time=0.62220\nEpoch   30: cost_train=17.4457550 cost_dev=18.3920269, time=0.63519\nEpoch   31: cost_train=17.4058743 cost_dev=18.3723087, time=0.62646\nEpoch   32: cost_train=17.3645802 cost_dev=18.3540611, time=0.69622\nEpoch   33: cost_train=17.3269444 cost_dev=18.3423405, time=0.66098\nEpoch   34: cost_train=17.2944489 cost_dev=18.3268318, time=0.62665\nEpoch   35: cost_train=17.2623348 cost_dev=18.3156242, time=0.62897\nEpoch   36: cost_train=17.2330284 cost_dev=18.3008232, time=0.70480\nEpoch   37: cost_train=17.2060337 cost_dev=18.2812634, time=0.63595\nEpoch   38: cost_train=17.1766911 cost_dev=18.2684097, time=0.68669\nEpoch   39: cost_train=17.1509285 cost_dev=18.2442646, time=0.67185\nEpoch   40: cost_train=17.1265888 cost_dev=18.2350349, time=0.65160\nEpoch   41: cost_train=17.1050491 cost_dev=18.2282124, time=0.66042\nEpoch   42: cost_train=17.0851898 cost_dev=18.2181034, time=0.63114\nEpoch   43: cost_train=17.0656853 cost_dev=18.2178898, time=0.63136\nEpoch   44: cost_train=17.0471535 cost_dev=18.2102623, time=0.62701\nEpoch   45: cost_train=17.0308228 cost_dev=18.1975765, time=0.62789\nEpoch   46: cost_train=17.0148811 cost_dev=18.1917095, time=0.63410\nEpoch   47: cost_train=16.9981308 cost_dev=18.1855049, time=0.62351\nEpoch   48: cost_train=16.9822407 cost_dev=18.1744461, time=0.62895\nEpoch   49: cost_train=16.9668064 cost_dev=18.1740437, time=0.62964\nEpoch   50: cost_train=16.9538422 cost_dev=18.1545601, time=0.62929\nEpoch   51: cost_train=16.9375858 cost_dev=18.1529942, time=0.62823\nEpoch   52: cost_train=16.9242001 cost_dev=18.1471081, time=0.63490\nEpoch   53: cost_train=16.9097481 cost_dev=18.1417732, time=0.63054\nEpoch   54: cost_train=16.8962460 cost_dev=18.1360569, time=0.63906\nEpoch   55: cost_train=16.8831367 cost_dev=18.1313705, time=0.62362\nEpoch   56: cost_train=16.8708477 cost_dev=18.1260128, time=0.62855\nEpoch   57: cost_train=16.8581219 cost_dev=18.1233025, time=0.63034\nEpoch   58: cost_train=16.8463249 cost_dev=18.1224518, time=0.63038\nEpoch   59: cost_train=16.8346138 cost_dev=18.1168747, time=0.62830\nEpoch   60: cost_train=16.8233604 cost_dev=18.1081066, time=0.62937\nEpoch   61: cost_train=16.8135300 cost_dev=18.1108227, time=0.64130\nEpoch   62: cost_train=16.8034477 cost_dev=18.1029320, time=0.62963\nEpoch   63: cost_train=16.7930241 cost_dev=18.0970955, time=0.62467\nEpoch   64: cost_train=16.7850857 cost_dev=18.0928593, time=0.62587\nEpoch   65: cost_train=16.7757187 cost_dev=18.0910549, time=0.62681\nEpoch   66: cost_train=16.7671871 cost_dev=18.0917397, time=0.62537\nEpoch   67: cost_train=16.7591076 cost_dev=18.0902405, time=0.62554\nEpoch   68: cost_train=16.7508965 cost_dev=18.0915146, time=0.62784\nEpoch   69: cost_train=16.7423153 cost_dev=18.0830612, time=0.63834\nEpoch   70: cost_train=16.7336922 cost_dev=18.0823040, time=0.62917\nEpoch   71: cost_train=16.7255020 cost_dev=18.0788193, time=0.62422\nEpoch   72: cost_train=16.7180424 cost_dev=18.0727806, time=0.62503\nEpoch   73: cost_train=16.7093716 cost_dev=18.0724983, time=0.62527\nEpoch   74: cost_train=16.7035770 cost_dev=18.0653934, time=0.63125\nEpoch   75: cost_train=16.6951370 cost_dev=18.0603313, time=0.65173\nEpoch   76: cost_train=16.6885357 cost_dev=18.0596352, time=0.63400\nEpoch   77: cost_train=16.6825619 cost_dev=18.0688496, time=0.62790\nEpoch   78: cost_train=16.6748505 cost_dev=18.0621834, time=0.62704\nEpoch   79: cost_train=16.6698341 cost_dev=18.0630112, time=0.62557\nEpoch   80: cost_train=16.6617393 cost_dev=18.0586109, time=0.62586\nEpoch   81: cost_train=16.6554928 cost_dev=18.0598392, time=0.63059\nEpoch   82: cost_train=16.6484032 cost_dev=18.0560627, time=0.62841\nEpoch   83: cost_train=16.6422482 cost_dev=18.0517902, time=0.63507\nEpoch   84: cost_train=16.6354675 cost_dev=18.0487862, time=0.62921\nEpoch   85: cost_train=16.6291103 cost_dev=18.0460815, time=0.62976\nEpoch   86: cost_train=16.6229248 cost_dev=18.0418987, time=0.63162\nEpoch   87: cost_train=16.6159534 cost_dev=18.0396729, time=0.63584\nEpoch   88: cost_train=16.6091099 cost_dev=18.0360508, time=0.63040\nEpoch   89: cost_train=16.6038532 cost_dev=18.0329800, time=0.62754\nEpoch   90: cost_train=16.5984402 cost_dev=18.0285282, time=0.62840\nEpoch   91: cost_train=16.5916805 cost_dev=18.0229015, time=0.67167\nEpoch   92: cost_train=16.5857735 cost_dev=18.0285263, time=0.63101\nEpoch   93: cost_train=16.5807476 cost_dev=18.0282555, time=0.62776\nEpoch   94: cost_train=16.5741577 cost_dev=18.0253277, time=0.63122\nEpoch   95: cost_train=16.5684242 cost_dev=18.0242481, time=0.62052\nEpoch   96: cost_train=16.5642891 cost_dev=18.0257950, time=0.63064\nEpoch   97: cost_train=16.5583763 cost_dev=18.0261230, time=0.63341\nEpoch   98: cost_train=16.5531006 cost_dev=18.0254745, time=0.63205\nEpoch   99: cost_train=16.5476971 cost_dev=18.0217838, time=0.62660\n"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    costs = []\n",
    "    for model, optimizer in zip(models, optimizers):\n",
    "        start = tf.timestamp()\n",
    "        cost_train, cost_dev = model.train(train_ds, dev_ds, optimizer)\n",
    "        end = tf.timestamp()\n",
    "        costs.append(cost_train)\n",
    "        costs.append(cost_dev)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch {0:4d}: cost_train={1:.7f} cost_dev={2:.7f}, time={3:.5f}'.format(total_epochs, cost_train, cost_dev,end-start))\n",
    "        #print('Epoch {0:4d}     {1}'.format(total_epochs, end-start))\n",
    "    total_epochs += 1\n",
    "\n",
    "    cost_history.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots = np.asarray(cost_history)\n",
    "plots = np.transpose(plots)\n",
    "m = plots.shape[1]\n",
    "plots = np.reshape(plots, [-1,2,m])\n",
    "for cost_train, cost_dev, model, color in zip(plots[:,0], plots[:,1], models, colors):\n",
    "    i = randint(0, len(colors)-1)\n",
    "    plt.plot(cost_train, color=colors[i], label=model.name+'-train')\n",
    "    plt.plot(cost_dev, color=colors[i], label=model.name+'-dev', linestyle='dashed')\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc accuracy\n",
    "for model in models:\n",
    "    print(model.name)\n",
    "    m = 0.\n",
    "    correct = 0.\n",
    "    for X,Y in train_ds:\n",
    "        prediction = model.forward(X)\n",
    "        prediction = tf.argmax(prediction, axis=0)\n",
    "        Y = tf.argmax(Y, axis=0)\n",
    "        temp = prediction == Y\n",
    "        temp = tf.cast(temp, tf.float32)\n",
    "        correct += tf.math.reduce_sum(temp)\n",
    "        m += tf.cast(tf.shape(X)[1], tf.float32)\n",
    "    tf.print('Train accuracy:',correct/m)\n",
    "\n",
    "    m = 0.\n",
    "    correct = 0.\n",
    "    for X,Y in dev_ds:\n",
    "        prediction = model.forward(X)\n",
    "        prediction = tf.argmax(prediction, axis=0)\n",
    "        Y = tf.argmax(Y, axis=0)\n",
    "        temp = prediction == Y\n",
    "        temp = tf.cast(temp, tf.float32)\n",
    "        correct += tf.math.reduce_sum(temp)\n",
    "        m += tf.cast(tf.shape(X)[1], tf.float32)\n",
    "    tf.print('Dev accuracy:',correct/m)\n",
    "    print('_____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_ds.take(1):\n",
    "    prediction = models[0].forward(X)\n",
    "    prediction = tf.math.argmax(prediction, axis=0)\n",
    "    ims = tf.reshape(X, [image_height, image_width, -1])\n",
    "\n",
    "offset = 10\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(ims[:,:,i+offset], cmap='binary')\n",
    "    plt.xlabel(prediction[i+offset].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = tf.io.read_file('test.png')\n",
    "img = tf.image.decode_png(f)\n",
    "img = img / 255\n",
    "img = tf.reshape(img, [image_height * image_width, -1])\n",
    "print(img.shape)\n",
    "tf.print(tf.math.reduce_max(img))\n",
    "plt.imshow(tf.reshape(img,[image_height, image_width]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "        prediction = model.forward(img)\n",
    "        tf.print('Prediction:',tf.argmax(prediction, axis=0))\n",
    "        for i in prediction:\n",
    "            tf.print(i)\n",
    "        print('_________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = network.predict(tf.linalg.matrix_transpose(img))\n",
    "print(prediction)\n",
    "tf.print(tf.math.argmax(prediction, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x, y in dev_ds.take(1):\n",
    "    test_img = x[:,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_img = tf.expand_dims(test_img, axis=1)\n",
    "plt.imshow(tf.reshape(test_img,[image_height, image_width]), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = models[0].forward(test_img)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.reshape(tf.math.abs(test_img-img),[image_height,image_width]), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = tf.image.convert_image_dtype(tf.reshape(test_img,[image_height,image_width,1]), tf.uint8)\n",
    "f = tf.image.encode_png(im)\n",
    "tf.io.write_file('image.png', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=[784,]),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "network.compile(optimizer='adam',\n",
    "              loss=Cost, metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network.fit(train_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}